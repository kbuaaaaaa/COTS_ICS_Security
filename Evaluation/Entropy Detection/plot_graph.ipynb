{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad00f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import entropy\n",
    "from matplotlib.collections import LineCollection\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080dcfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['iforest','histogram','pca','svm','cluster']\n",
    "score = []\n",
    "for file in models:\n",
    "    y_pred = pd.read_csv('Plot/os/result/os_anomaly_value_'+file+'.csv')\n",
    "    y_true = pd.read_csv('Plot/os/result/os_true_value_'+file+'.csv')\n",
    "    score.append([file, 'precision', precision_score(y_true, y_pred, average='macro')])\n",
    "    score.append([file, 'recall', recall_score(y_true, y_pred, average='macro')])\n",
    "    score.append([file, 'f1', f1_score(y_true, y_pred, average='macro')])\n",
    "    score.append([file, 'accuracy', accuracy_score(y_true, y_pred)])\n",
    "    \n",
    "score = pd.DataFrame(score)\n",
    "score.columns = ['model','score','value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4655b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='model', y='value', hue='score', data=score, kind='bar', height=5, aspect=1.5).set(title =\"Performance score comparison\")\n",
    "plt.ylim([0,1])\n",
    "plt.savefig('Plot/os/performance/comparison.pdf',bbox_inches='tight') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd949d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,1)\n",
    "for file in models:\n",
    "    y_pred = pd.read_csv('Plot/os/result/os_anomaly_value_'+file+'.csv')\n",
    "    y_true = pd.read_csv('Plot/os/result/os_true_value_'+file+'.csv')\n",
    "    plot = RocCurveDisplay.from_predictions(y_true,y_pred,pos_label=1,name=file,linewidth=1.5,ax = axs)\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC curve comparison\")\n",
    "plt.savefig('Plot/os/performance/roc_auc.pdf') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54690b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['iforest','histogram','pca','svm','cluster']\n",
    "score = []\n",
    "for file in models:\n",
    "    y_pred = pd.read_csv('Plot/ics/result/all_anomaly_value_'+file+'.csv')\n",
    "    y_true = pd.read_csv('Plot/ics/result/all_true_value_'+file+'.csv')\n",
    "    score.append([file, 'precision', precision_score(y_true, y_pred, average='macro')])\n",
    "    score.append([file, 'recall', recall_score(y_true, y_pred, average='macro')])\n",
    "    score.append([file, 'f1', f1_score(y_true, y_pred, average='macro')])\n",
    "    score.append([file, 'accuracy', accuracy_score(y_true, y_pred)])\n",
    "    \n",
    "score = pd.DataFrame(score)\n",
    "score.columns = ['model','score','value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7710935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='model', y='value', hue='score', data=score, kind='bar', height=5, aspect=1.5).set(title =\"Performance score comparison\")\n",
    "plt.savefig('Plot/ics/performance/comparison.pdf',bbox_inches='tight') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f471ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,1)\n",
    "for file in models:\n",
    "    y_pred = pd.read_csv('Plot/ics/result/all_anomaly_value_'+file+'.csv')\n",
    "    y_true = pd.read_csv('Plot/ics/result/all_true_value_'+file+'.csv')\n",
    "    plot = RocCurveDisplay.from_predictions(y_true,y_pred,pos_label=1,name=file,linewidth=1.5,ax = axs)\n",
    "\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC curve comparison\")\n",
    "plt.savefig('Plot/ics/performance/roc_auc.pdf') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fb387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in models:\n",
    "    y_pred = pd.read_csv('Plot/ics/result/all_anomaly_value_'+file+'.csv')\n",
    "    y_true = pd.read_csv('Plot/ics/result/all_true_value_'+file+'.csv')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.title(file+\" confusion matrix\")\n",
    "    plt.savefig('Plot/ics/performance/confusion_'+file+'.pdf') \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5dea00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb099ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c10e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['arp','combination','injection','readbomb','recon','synflood']\n",
    "for file in files:\n",
    "    y_pred = pd.read_csv('Plot/'+file+'/result/anomaly_value_iforest.csv')\n",
    "    y_true = pd.read_csv('Plot/'+file+'/result/true_value_iforest.csv')\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()\n",
    "    plt.title(\"Iforest confusion matrix on \"+file)\n",
    "    plt.savefig('Plot/'+file+'/performance/confusion_iforest.pdf') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c22aea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ce92a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['arp','combination','injection','readbomb','recon','synflood']\n",
    "score = []\n",
    "for file in files:\n",
    "    y_pred = pd.read_csv('plot/'+file+'_anomaly_value.csv')\n",
    "    y_true = pd.read_csv('plot/'+file+'_true_value.csv')\n",
    "    score.append([file, 'precision', precision_score(y_true, y_pred, average='macro')])\n",
    "    score.append([file, 'recall', recall_score(y_true, y_pred, average='macro')])\n",
    "    score.append([file, 'f1', f1_score(y_true, y_pred, average='macro')])\n",
    "    score.append([file, 'accuracy', accuracy_score(y_true, y_pred)])\n",
    "    \n",
    "score = pd.DataFrame(score)\n",
    "score.columns = ['attack','score','value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854b8f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='attack', y='value', hue='score', data=score, kind='bar', height=4, aspect=2,)\n",
    "plt.savefig('Plot/ics/performance/comparison.pdf',bbox_inches='tight') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef26fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['arp','combination','injection','readbomb','recon','synflood']\n",
    "models = ['iforest','histogram','pca','svm','cluster']\n",
    "for file in files:\n",
    "    score = []\n",
    "    for m in models:\n",
    "        y_pred = pd.read_csv('Plot/'+file+'/result/anomaly_value_'+m+'.csv')\n",
    "        y_true = pd.read_csv('Plot/'+file+'/result/true_value_'+m+'.csv')\n",
    "        score.append([m, 'precision', precision_score(y_true, y_pred, average='macro')])\n",
    "        score.append([m, 'recall', recall_score(y_true, y_pred, average='macro')])\n",
    "        score.append([m, 'f1', f1_score(y_true, y_pred, average='macro')])\n",
    "        score.append([m, 'accuracy', accuracy_score(y_true, y_pred)])\n",
    "\n",
    "    score = pd.DataFrame(score)\n",
    "    score.columns = ['model','score','value']\n",
    "\n",
    "    sns.catplot(x='model', y='value', hue='score', data=score, kind='bar', height=5, aspect=1.5).set(title =\"Performance score comparison\")\n",
    "    plt.savefig('Plot/'+file+'/performance/comparison.pdf',bbox_inches='tight') \n",
    "    print(file, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7be3f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['arp','combination','injection','readbomb','recon','synflood']\n",
    "models = ['iforest','histogram','pca','svm','cluster']\n",
    "for file in files:\n",
    "    fig, axs = plt.subplots(1,1)\n",
    "    for m in models:\n",
    "        y_pred = pd.read_csv('Plot/'+file+'/result/anomaly_value_'+m+'.csv')\n",
    "        y_true = pd.read_csv('Plot/'+file+'/result/true_value_'+m+'.csv')\n",
    "        plot = RocCurveDisplay.from_predictions(y_true,y_pred,pos_label=1,name=m,linewidth=1.5,ax = axs)\n",
    "\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC curve comparison\")\n",
    "    plt.savefig('Plot/'+file+'/performance/roc_auc.pdf') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d43d86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
